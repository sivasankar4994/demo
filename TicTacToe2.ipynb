{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TicTacToe2.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"BgGkr3XEAsWa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":123},"outputId":"3cd39615-379a-4f99-bbfa-e8fe968c3d3c"},"cell_type":"code","source":["import tensorflow as tf\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten\n","from keras.models import load_model\n","from keras import optimizers\n","import random\n","import numpy as np\n","import math\n","\n","reward_dep = .9\n","x_train = True\n","\n","model = Sequential()\n","model.add(Dense(units=130, activation='relu', input_dim=27, kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model.add(Dense(units=250, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model.add(Dense(units=140, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model.add(Dense(units=60, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model.add(Dense(9, kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n","\n","model_2 = Sequential()\n","model_2.add(Dense(units=130, activation='relu', input_dim=27, kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model_2.add(Dense(units=250, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model_2.add(Dense(units=140, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model_2.add(Dense(units=60, activation='relu', kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model_2.add(Dense(9, kernel_initializer='random_uniform', bias_initializer='zeros'))\n","model_2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n","\n","def onehot_enc(state):\n","\tcurrent_state = []\n","\n","\tfor square in state:\n","\t\tif square == 0:\n","\t\t\tcurrent_state.append(1)\n","\t\t\tcurrent_state.append(0)\n","\t\t\tcurrent_state.append(0)\n","\t\telif square == 1:\n","\t\t\tcurrent_state.append(0)\n","\t\t\tcurrent_state.append(1)\n","\t\t\tcurrent_state.append(0)\n","\t\telif square == -1:\n","\t\t\tcurrent_state.append(0)\n","\t\t\tcurrent_state.append(0)\n","\t\t\tcurrent_state.append(1)\n","\n","\treturn current_state\n","\n","def outcome_game(state):\n","\ttotal_reward = 0\n","\n","\tfor i in range(0, 9):\n","\t\tif i == 0 or i == 3 or i == 6:\n","\t\t\tif state[i] == state[i + 1] and state[i] == state[i + 2]:\n","\t\t\t\ttotal_reward = state[i]\t\t\t\t\t\n","\t\t\t\tbreak\n","\t\t\telif state[0] == state[4] and state[0] == state[8] and i == 0:\n","\t\t\t\ttotal_reward = state[0]\n","\t\t\t\tbreak\n","\t\tif i < 3:\n","\t\t\tif state[i] == state[i + 3] and state[i] == state[i + 6]:\n","\t\t\t\ttotal_reward = state[i]\t\t\t\t\t\n","\t\t\t\tbreak\n","\t\t\telif state[2] == state[4] and state[2] == state[6] and i == 2:\n","\t\t\t\ttotal_reward = state[2]\n","\t\t\t\tbreak\n","\n","\tif (state[0] == state[1] == state[2]) and not state[0] == 0:\n","\t\ttotal_reward = state[0]\t\n","\telif (state[3] == state[4] == state[5]) and not state[3] == 0:\n","\t\ttotal_reward = state[3]\t\n","\telif (state[6] == state[7] == state[8]) and not state[6] == 0:\n","\t\ttotal_reward = state[6]\t\n","\telif (state[0] == state[3] == state[6]) and not state[0] == 0:\n","\t\ttotal_reward = state[0]\t\n","\telif (state[1] == state[4] == state[7]) and not state[1] == 0:\n","\t\ttotal_reward = state[1]\t\n","\telif (state[2] == state[5] == state[8]) and not state[2] == 0:\n","\t\ttotal_reward = state[2]\t\n","\telif (state[0] == state[4] == state[8]) and not state[0] == 0:\n","\t\ttotal_reward = state[0]\t\n","\telif (state[2] == state[4] == state[6]) and not state[2] == 0:\n","\t\ttotal_reward = state[2]\n","\n","\treturn total_reward\n","\n","try:\n","\tmodel = load_model('tic_tac_toe.h5')\n","\tmodel_2 = load_model('tic_tac_toe_2.h5')\n","\tprint('Pre-existing model found... loading data.')\n","except:\n","\tpass\n","\n","def process_models(games, model, model_2):\n","\tglobal x_train\n","\txt = 0\n","\tot = 0\n","\tdt = 0\n","\tstates = []\n","\tq_values = []\n","\tstates_2 = []\n","\tq_values_2 = []\n","\n","\tfor game in games:\n","\t\ttotal_reward = outcome_game(game[len(game) - 1])\n","\t\tif total_reward == -1:\n","\t\t\tot += 1\n","\t\telif total_reward == 1:\n","\t\t\txt += 1\n","\t\telse:\n","\t\t\tdt += 1\n","\t\t# print('------------------')\n","\t\t# print(game[len(game) - 1][0], game[len(game) - 1][1], game[len(game) - 1][2])\n","\t\t# print(game[len(game) - 1][3], game[len(game) - 1][4], game[len(game) - 1][5])\n","\t\t# print(game[len(game) - 1][6], game[len(game) - 1][7], game[len(game) - 1][8])\n","\t\t# print('reward =', total_reward)\n","\n","\t\tfor i in range(0, len(game) - 1):\n","\t\t\tif i % 2 == 0:\n","\t\t\t\tfor j in range(0, 9):\n","\t\t\t\t\tif not game[i][j] == game[i + 1][j]:\n","\t\t\t\t\t\treward_vector = np.zeros(9)\n","\t\t\t\t\t\treward_vector[j] = total_reward*(reward_dep**(math.floor((len(game) - i) / 2) - 1))\n","\t\t\t\t\t\t# print(reward_vector)\n","\t\t\t\t\t\tstates.append(game[i].copy())\n","\t\t\t\t\t\tq_values.append(reward_vector.copy())\n","\t\t\telse:\n","\t\t\t\tfor j in range(0, 9):\n","\t\t\t\t\tif not game[i][j] == game[i + 1][j]:\n","\t\t\t\t\t\treward_vector = np.zeros(9)\n","\t\t\t\t\t\treward_vector[j] = -1*total_reward*(reward_dep**(math.floor((len(game) - i) / 2) - 1))\n","\t\t\t\t\t\t# print(reward_vector)\n","\t\t\t\t\t\tstates_2.append(game[i].copy())\n","\t\t\t\t\t\tq_values_2.append(reward_vector.copy())\n","\n","\tif x_train:\n","\t\tzipped = list(zip(states, q_values))\n","\t\trandom.shuffle(zipped)\n","\t\tstates, q_values = zip(*zipped)\n","\t\tnew_states = []\n","\t\tfor state in states:\n","\t\t\tnew_states.append(onehot_enc(state))\n","\n","\t\t# for i in range(0, len(states)):\n","\t\t\t# print(new_states[i], states[i], q_values[i])\n","\t\t\t# print(np.asarray(new_states))\n","\n","\t\tmodel.fit(np.asarray(new_states), np.asarray(q_values), epochs=4, batch_size=len(q_values), verbose=1)\n","\t\tmodel.save('tic_tac_toe.h5')\n","\t\tdel model\n","\t\tmodel = load_model('tic_tac_toe.h5')\n","\t\tprint(xt/20, ot/20, dt/20)\n","\telse:\n","\t\tzipped = list(zip(states_2, q_values_2))\n","\t\trandom.shuffle(zipped)\n","\t\tstates_2, q_values_2 = zip(*zipped)\n","\t\tnew_states = []\n","\t\tfor state in states_2:\n","\t\t\tnew_states.append(onehot_enc(state))\n","\n","\t\t# for i in range(0, len(states)):\n","\t\t\t# print(new_states[i], states[i], q_values[i])\n","\t\t\t# print(np.asarray(new_states))\n","\n","\t\tmodel_2.fit(np.asarray(new_states), np.asarray(q_values_2), epochs=4, batch_size=len(q_values_2), verbose=1)\n","\t\tmodel_2.save('tic_tac_toe_2.h5')\n","\t\tdel model_2\n","\t\tmodel_2 = load_model('tic_tac_toe_2.h5')\n","\t\tprint(xt/20, ot/20, dt/20)\n","\n","\tx_train = not x_train\n","\n","# win = 1; draw = 0; loss = -1 --> moves not taken are 0 in q vector\n","\n","\n","\n","mode = input('Choose a mode: (training/playing) ')\n","no_times=0\n","while True:\n","\tboard = [0, 0, 0, 0,  0, 0, 0, 0, 0]\n","\t# sides --> 0 = Os, 1 = Xs\n","\tgames = []\n","\tcurrent_game = []\n","\n","\tif mode == 'training':\n","\t\tprint(x_train)\n","\t\t# total_games = int(input('How many games should be played? '))\n","\t\ttotal_games = 2000\n","\t\t# e_greedy = float(input('What will the epsilon-greedy value be? '))\n","\t\te_greedy = .7\n","\n","\t\tfor i in range(0, total_games):\n","# \t\t\tprint(i)\n","\t\t\tplaying = True\n","\t\t\tnn_turn = True\n","\t\t\tc = 0\n","\t\t\tboard = [0, 0, 0, 0,  0, 0, 0, 0, 0]\n","\t\t\t# sides --> 0 = Os, 1 = Xs\n","\t\t\tcurrent_game = []\n","\t\t\tcurrent_game.append(board.copy())\n","\t\t\tnn_board = board\n","\n","\t\t\twhile playing:\n","\t\t\t\tif nn_turn:\n","\t\t\t\t\tif random.uniform(0, 1) <= e_greedy:\n","\t\t\t\t\t\tchoosing = True\n","\t\t\t\t\t\twhile choosing:\n","\t\t\t\t\t\t\tc = random.randint(0, 8)\n","\t\t\t\t\t\t\tif board[c] == 0:\n","\t\t\t\t\t\t\t\tchoosing = False\n","\t\t\t\t\t\t\t\tboard[c] = 1\n","\t\t\t\t\t\t\t\tcurrent_game.append(board.copy())\n","\t\t\t\t\t\t\t\t# save state to game array\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tpre = model.predict(np.asarray([onehot_enc(board)]), batch_size=1)[0]\n","\t\t\t\t\t\thighest = -1000\n","\t\t\t\t\t\tnum = -1\n","\t\t\t\t\t\tfor j in range(0, 9):\n","\t\t\t\t\t\t\tif board[j] == 0:\n","\t\t\t\t\t\t\t\tif pre[j] > highest:\n","\t\t\t\t\t\t\t\t\thighest = pre[j].copy()\n","\t\t\t\t\t\t\t\t\tnum = j\n","\n","\t\t\t\t\t\tchoosing = False\n","\t\t\t\t\t\tboard[num] = 1\n","\t\t\t\t\t\tcurrent_game.append(board.copy())\n","\n","\t\t\t\telse:\n","\t\t\t\t\tif random.uniform(0, 1) <= e_greedy:\n","\t\t\t\t\t\tchoosing = True\n","\t\t\t\t\t\twhile choosing:\n","\t\t\t\t\t\t\tc = random.randint(0, 8)\n","\t\t\t\t\t\t\tif board[c] == 0:\n","\t\t\t\t\t\t\t\tchoosing = False\n","\t\t\t\t\t\t\t\tboard[c] = -1\n","\t\t\t\t\t\t\t\tcurrent_game.append(board.copy())\n","\t\t\t\t\t\t\t\t# save state to game array\n","\t\t\t\t\telse:\n","\t\t\t\t\t\tpre = model_2.predict(np.asarray([onehot_enc(board)]), batch_size=1)[0]\n","\t\t\t\t\t\thighest = -1000\n","\t\t\t\t\t\tnum = -1\n","\t\t\t\t\t\tfor j in range(0, 9):\n","\t\t\t\t\t\t\tif board[j] == 0:\n","\t\t\t\t\t\t\t\tif pre[j] > highest:\n","\t\t\t\t\t\t\t\t\thighest = pre[j].copy()\n","\t\t\t\t\t\t\t\t\tnum = j\n","\n","\t\t\t\t\t\tchoosing = False\n","\t\t\t\t\t\tboard[num] = -1\n","\t\t\t\t\t\tcurrent_game.append(board.copy())\n","\n","\t\t\t\tplayable = False\n","\n","\t\t\t\tfor square in board:\n","\t\t\t\t\tif square == 0:\n","\t\t\t\t\t\tplayable = True\n","\t\t\t\t\t# elif find square and check\n","\n","\t\t\t\tif not outcome_game(board) == 0:\n","\t\t\t\t\tplayable = False\n","\n","\t\t\t\t#print(outcome_game(board))\n","\n","\t\t\t\tif not playable:\n","\t\t\t\t\tplaying = False\n","\n","\t\t\t\tnn_turn = not nn_turn\n","\n","\t\t\t\t# print(board[0], board[1], board[2])\n","\t\t\t\t# print(board[3], board[4], board[5])\n","\t\t\t\t# print(board[6], board[7], board[8])\n","\n","\t\t\tgames.append(current_game)\n","\t\t\t# print('current game:', current_game)\n","\n","\t\tprocess_models(games, model, model_2)\n","\telif mode == 'playing':\n","\t\tprint('')\n","\t\tprint('A new game is starting!')\n","\t\tprint('')\n","\n","\t\tteam = input('Choose a side: (x/o) ')\n","\t\tprint('')\n","\n","\t\tboard = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\t\trunning = True\n","\t\tx_turn = True\n","\t\twhile running:\n","\t\t\tif (x_turn and team == 'o') or (not x_turn and not team == 'o'):\n","\t\t\t\tif team == 'o':\n","\t\t\t\t\tpre = model.predict(np.asarray([onehot_enc(board)]), batch_size=1)[0]\n","\t\t\t\telif team == 'x':\n","\t\t\t\t\tpre = model_2.predict(np.asarray([onehot_enc(board)]), batch_size=1)[0]\n","\t\t\t\t# print(pre)\n","\t\t\t\tprint('')\n","\t\t\t\thighest = -1000\n","\t\t\t\tnum = -1\n","\t\t\t\tfor j in range(0, 9):\n","\t\t\t\t\tif board[j] == 0:\n","\t\t\t\t\t\tif pre[j] > highest:\n","\t\t\t\t\t\t\thighest = pre[j].copy()\n","\t\t\t\t\t\t\tnum = j\n","\n","\t\t\t\tprint(pre)\n","\n","\t\t\t\t# TODO: ADD EXTRA IF STATEMENT FOR NUM == -1 (FIRST OPTION ALWAYS TRUMPS)\n","\n","\t\t\t\tif team == 'o':\n","\t\t\t\t\tboard[num] = 1\n","\t\t\t\telif team == 'x':\n","\t\t\t\t\tboard[num] = -1\n","\t\t\t\tx_turn = not x_turn\n","\t\t\t\tprint('AI is thinking...')\n","\t\t\telse:\n","\t\t\t\tmove = int(input('Input your move: '))\n","\t\t\t\tif board[move] == 0:\n","\t\t\t\t\tif team == 'o':\n","\t\t\t\t\t\tboard[move] = -1\n","\t\t\t\t\telif team == 'x':\n","\t\t\t\t\t\tboard[move] = 1\n","\t\t\t\t\tx_turn = not x_turn\n","\t\t\t\telse:\n","\t\t\t\t\tprint('Invalid move!')\n","\n","\t\t\tr_board = []\n","\n","\t\t\tfor square in board:\n","\t\t\t\tif square == 0:\n","\t\t\t\t\tr_board.append('-')\n","\t\t\t\telif square == 1:\n","\t\t\t\t\tr_board.append('x')\n","\t\t\t\telif square == -1:\n","\t\t\t\t\tr_board.append('o')\n","\n","\t\t\tprint(r_board[0], r_board[1], r_board[2])\n","\t\t\tprint(r_board[3], r_board[4], r_board[5])\n","\t\t\tprint(r_board[6], r_board[7], r_board[8])\n","\t\t\t\n","\t\t\tfull = True\n","\t\t\tno_times+=1\n","\n","\t\t\tfor square in board:\n","\t\t\t\tif square == 0:\n","\t\t\t\t\tfull = False\n","\n","\t\t\tif full:\n","\t\t\t\trunning = False\n","\t\t\t\tif outcome_game(board) == 0:\n","\t\t\t\t\tprint('The game was drawn!')\n","\n","\t\t\tif not outcome_game(board) == 0:\n","\t\t\t\trunning = False\n","\t\t\t\tprint(outcome_game(board), 'won the game!')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"MVCn50AjAyh7","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}